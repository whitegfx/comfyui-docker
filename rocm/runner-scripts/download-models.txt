# Since SD models are so diverse now (SD 1.5, SDXL, FLUX.1, etc.),
# I have commented out most download links to give the choice to users.

# TAESD

https://raw.githubusercontent.com/madebyollin/taesd/main/taesd_decoder.pth
  dir=vae_approx
  out=taesd_decoder.pth
https://raw.githubusercontent.com/madebyollin/taesd/main/taesdxl_decoder.pth
  dir=vae_approx
  out=taesdxl_decoder.pth
https://raw.githubusercontent.com/madebyollin/taesd/main/taesd3_decoder.pth
  dir=vae_approx
  out=taesd3_decoder.pth
https://raw.githubusercontent.com/madebyollin/taesd/main/taef1_decoder.pth
  dir=vae_approx
  out=taef1_decoder.pth

# Checkpoints

#https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/resolve/main/playground-v2.5-1024px-aesthetic.fp16.safetensors
#  dir=checkpoints
#  out=playground-v2.5-1024px-aesthetic.fp16.safetensors

# VAE

#https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors
#  dir=vae
#  out=vae-ft-mse-840000-ema-pruned.safetensors

# Upscale

#https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth
#  dir=upscale_models
#  out=RealESRGAN_x4plus.pth
#https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth
#  dir=upscale_models
#  out=RealESRGAN_x4plus_anime_6B.pth
#https://huggingface.co/Kim2091/AnimeSharp/resolve/main/4x-AnimeSharp.pth
#  dir=upscale_models
#  out=4x-AnimeSharp.pth
#https://huggingface.co/Kim2091/UltraSharp/resolve/main/4x-UltraSharp.pth
#  dir=upscale_models
#  out=4x-UltraSharp.pth
#https://huggingface.co/gemasai/4x_NMKD-Siax_200k/resolve/main/4x_NMKD-Siax_200k.pth
#  dir=upscale_models
#  out=4x_NMKD-Siax_200k.pth
#https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/8x_NMKD-Superscale_150000_G.pth
#  dir=upscale_models
#  out=8x_NMKD-Superscale_150000_G.pth

# Embeddings

#https://huggingface.co/datasets/gsdf/EasyNegative/resolve/main/EasyNegative.safetensors
#  dir=embeddings
#  out=easynegative.safetensors
#https://huggingface.co/lenML/DeepNegative/resolve/main/NG_DeepNegative_V1_75T.pt
#  dir=embeddings
#  out=ng_deepnegative_v1_75t.pt

# CLIP Vision

#https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/model.safetensors
#  dir=clip_vision
#  out=clip_vit14.safetensors

# ControlNet v1.1
# More models: https://huggingface.co/lllyasviel/sd_control_collection

#https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth
#  dir=controlnet
#  out=control_v11f1p_sd15_depth.pth
#https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_canny.pth
#  dir=controlnet
#  out=control_v11p_sd15_canny.pth
#https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_openpose.pth
#  dir=controlnet
#  out=control_v11p_sd15_openpose.pth
